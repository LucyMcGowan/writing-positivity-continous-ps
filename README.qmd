---
title: "The bad news about non-structural lack of positivity for continuous exposures"
format: gfm
bibliography: citations.bib
---

## Introduction

The formal goal of much research is to investigate and quantify causal relationships between variables of interest. Methods for drawing causal inference from data have seen widespread development and popularity over the last few decades [@imbens2015causal ; @morgan2015counterfactuals ; @glymour2016causal ; @hernan2020what]. However, the majority of causal methods are defined for settings with discrete-valued (typically binary) exposures. Despite the high prevalence of continuous-valued exposures in scientific research, existing methods for continuous exposures are underdeveloped and understudied compared to their discrete counterparts.

The discrete vs. continuous nature of an exposure of interest can require modest modifications to some methods, but fairly large modifications to others. For an example of the former, consider outcome model-based approaches, such as g-computation [@robins1986new ; @snowden2011implementation]. G-computation entails fitting a model for the outcome conditional on exposure and confounders, and marginalizing over the confounder variables. This process is agnostic to whether the exposure is discrete or continuous, although in practice the outcome model may be much harder to specify correctly when the exposure is continuous. For an example of the latter, consider propensity score-based approaches, such as inverse-probability weighting (IPW) [@robins2000marginal ; @cole2008constructing]. IPW methods entail fitting a model for the exposure conditional on confounders and weighting subjects according to their ``propensity'' to receive their exposure value. This propensity is typically defined as a probability for discrete exposure and a density for continuous exposures [@naimi2014constructing], introducing fairly large changes to the method in both its theoretical justification and its usage in practice. Understanding the conditions under which each of these commonly used approaches is more suitable for an analysis is key to informing statistical practice. However, the finite-sample properties of causal methods for continuous exposures are under-explored, particularly in settings where their underlying assumptions may be violated.

The assumptions for drawing causal inference from data are related to the *assignment mechanism*. There are three key assumptions: (1) the assignment is *individualistic*, that is, an individual's assignment probability cannot depend on the values of covariates or potential outcomes of other individuals; (2) the assignment is *unconfounded*, i.e., the assignment mechanism cannot depend on the potential outcomes; and (3) the assignment is *probabilistic*, such that there is a non-zero probability for each treatment value for every individual. [@imbens2015causal] This final assumption, sometimes referred to as *positivity*, has been shown to be a sensitivity point for methods in the discrete exposure setting, even when the assumption is known to hold but the number of individuals for certain exposure-covariate combinations is small [@peterson2012diagnosing ; @leger2022causal]. This paper explores how sensitive popular causal methods are to similar non-structural positivity violations in the continuous exposure setting.

The structure of this paper is as follows. In Section 2, we define notation and both standard and generalized propensity scores. In Section 3, we define the primary methods of interest and compare them under a variety of data generating mechanisms corresponding to varying strengths of non-structural positivity violations. In Section 4, we apply the methods to a real data application where positivity is questionable. Section 5 concludes with discussion and recommendations for practitioners.

## Notation and propensity scores

Denote the continuous exposure as $T$, which takes on values in a set $\mathscr{T}$. For individual $i$ and each value of the exposure, $t$, there is a potential outcome $Y_i(t)$. [@imbens2000role] Each individual also has a vector of pre-exposure covariates, $X_i$. In the most general case, the assignment mechanism is defined as $P(T=t|\mathbf{X}, \mathbf{Y}(t))$; with the unconfounded assumption this reduces to $P(T=t|\mathbf{X})$. This definition of the assignment mechanism is commonly referred to as a propensity score. An assignment mechanism is *probabilisitic* if the probability of assignment to each exposure level is strictly between 0 and 1:

$$0 < P_i(X, Y(t)) < 1, \textrm{ for each possible } X, Y(t)$$ 

for all $i=1, \dots, N$. In other words, the probabilistic (positivity) assumption states that for all individuals within strata of $X$ there exists a non-zero probability of receiving every exposure level.

A generalized propensity score relaxes the idea of propensity scores to the continuous exposure setting and is defined as
$$e(t, x) = f_{t|x}(T = t | X = x)$$
Thus, the assignment mechanism is represented as a conditional density rather than as a probability. The positivity assumption is defined similarly, such that the conditional density of each exposure level must be greater than 0 for all possible confounder levels, i.e., $e(t, x) > 0$, for all $x$ where $f(X=x) > 0$.

Propensity scores have a balancing property such that blah. This allows them to be used to estimate causal effects in the presence of measured confounding. A variety of methods exist to leverage propensity scores for this purpose, including matching, weighting, conditional modeling, stratification, and doubly robust approaches (add citations). For the purposes of this paper, we will focus attention on the popular method of propensity score weighting, and the sentivity of this method to positivity violations.

Violations (or near violations) of the positivity assumption can increase both the bias and variance of causal effect estimates. [@petersen2012diagnosing]. There are two ways this violation can arise: structural non-positivity and random non-positivity. The former suggests that there is a structural mechanism that makes certain levels of the exposure impossible for a subset of individuals. The latter are random violations that can occur in finite samples due to chance. For example, an assignment mechanism that is truly normally distributed would not yield a structural positivity violation (as the support of the exposure variable would be all real numbers across the set of possible confounder values), but it could very likely yield a random non-positivity violation as the conditional normal distribution may have sparse overlap for many levels of the confounder(s). This paper will focus on random violations of positivity, demonstrating that "finite" does not mean small, but rather in the case of continuous exposures can be any sample size that is not infinite.

## Simulations

The aim of this simulation study is to compare popular causal inference methods for continuous exposures under settings of random violations to the positivity assumption. First we define the estimand of interest.

### Estimand

Consider a marginal structural model (MSM) for the potential outcomes,
$$ E\{Y(t)\} = f(\beta, t) $$

where a finite constraint on $\beta$ places parametric restrictions on the model. For this simulation, we will consider a simple MSM, namely $E\{Y(t)\} = \beta_{0} + \beta_{1}t$. The estimand of interest is then the coefficient $\beta_{1}$, which summarizes the causal effect of $T$ on $Y$.

### Methods

Three methods will be considered in simulation studies. The primary method of interest is a propensity score weighted estimator. First one must estimate the propensity scores by fitting a model with the exposure variable as the model outcome, conditional on any assumed confounder variables (or generally, a set of baseline pre-exposure variables which includes potential confounders). In practice, a linear model assuming that the exposure is conditionally normally distributed is a common choice, although other methods are available. [@naimi2014constructing] We then use the fitted values and the model variance in a Gaussian probability density function. [@austin2019assessing] 

$$e(t, x) = f_{T|X}(t|x) = \frac{1}{\sqrt{2\pi\hat\sigma^2}}\exp\left\{-\frac{(t-X\hat\beta)^2}{2\pi\hat\sigma^2}\right\}$$
The following stabilized weight is then used, where the numerator is the marginal density of the exposure.

$$w = \frac{f_T(t)}{f_{T|X}(t|x)} = \frac{\hat{\sigma}_{t|x}}{\hat\sigma_t}\exp\left\{\frac{(t-X\hat\beta)^2}{2\hat\sigma_{t|x}^2}-\frac{(t-\mu_t)^2}{2\hat\sigma_t^2}\right\}$$
Finally, a second model is fit, this time of the outcome $Y$ conditional on the exposure $T$, but where each observation is weighted by the corresponding stabilized weight $w$. Then the coefficient of the exposure in this fitted model provides an estimator of the causal effect of interest $\beta_{1}$.

Two comparator methods will be considered, namely unadjusted and adjusted regression models. Unadjusted models will fit a model of outcome on exposure, without conditioning on any covariates. Adjusted models will fit a correctly specified model conditional on exposure and covariates. Note that such adjusted models are equivalent to the method of standardization over covariates, as long as there are no interaction terms between exposure and covariates. In each case, the coefficient of the exposures in the fitted models will act as comparison estimators for the causal effect of interest.

### Data generating mechanisms

We examine 3 scenarios:

1. A single binary confounder with a moderate effect on the exposure and outcome
2. A single binary confounder with a large effect on the exposure and moderate effect on the outcome
3. A single binary confounder with a large effect on the exposure and large effect on the outcome.

We generate a binary confounder, $X$, from a Bernoulli distribution with probability 0.5. The continuous exposure, $T$ is generated as follows:

$$
T = aX + \varepsilon_{t|x}
$$ {#eq-t}
Where $\varepsilon_{t|x}\sim N(0,1)$ and $Y$ such that the "true" effect of $T$ is 0:

$$
Y = bX + \varepsilon_{y|x}
$$ {#eq-y}

where $\varepsilon_{y|x}\sim N(0,1)$. We examine $a = 1, 10$ and $b=1,10$ for moderate and large effects, respectively. We vary the sample size from 100 to 1,000,000, examining 100 replicates of each.

### Results

When $a = 1$, the positivity assumption is not violated; for example @fig-a1 shows a mirrored histogram for a simulation as specified above with a sample size of 10,000. When $a = 10$, however, we do see a near positivity violation, for all sample sizes. Here, because $T$ is continuous, the individuals at the population level *do* have a non-zero probability of receiving all other values of the exposure, however in practice it happens very infrequently. See @fig-a2, a mirrored histogram for a simulation where $a=10$ and a sample size of 10,000.

```{r}
#| label: "fig-a1"
#| fig-cap: "Mirrored Histogram showing overlap. a = 1, b = 1, n = 10,000"
#| echo: false
library(targets)
tar_read(fig_a1)
```

```{r}
#| label: fig-a2
#| fig-cap: "Mirrored histogram with positivity near-violation"
#| echo: false
tar_read(fig_a2)
```

```{r}
#| label: fig-weight
#| fig-cap: "oh no. Weighted pseudopopulation using GPS (ATE) weights"
#| echo: false
tar_read(fig_weight)
```

The estimates for the effect of the treatment are biased. The distribution for the effect is non-normal (@fig-skew).

```{r}
#| label: fig-skew
#| fig-cap: "Skewed distribution of the estimated coefficient for the treatment. For reference a normal density is overlaid in orange."
#| echo: false
tar_read(fig_skew)
```

@fig-sims demonstrates that in the case of a continuous exposure, when there is near violation of the positivity assumption, as seen in panels B and C, the causal estimate using the stabilized generalized propensity score is both biased and has large variance, which does not appreciably decrease despite the large sample size.


```{r}
#| label: fig-sims
#| fig-height: 10
#| message: false
#| warning: false
#| fig-cap: "Boo. Look at all that bias and variance"
#| echo: false
tar_read(fig_sims)
```

@fig-coverage shows similar results for a coverage metric, with some undercoverage even observable in Panel A.

```{r}
#| label: fig-coverage
#| fig-height: 10
#| fig-cap: "The coverage ain't coveraging"
#| echo: false
tar_read(fig_coverage)
```

The magnitude of the bias and variability depend on the magnitude of the effect of $X$ and $T$ (in @eq-t as $a$), and, for a binary confounder the prevalence of the confounder ($p$). Below is a heatmap exploring the relationship between these two quantities when $n = 10,000$

```{r}
#| label: fig-bias
#| message: false
#| warning: false
#| fig-cap: "Impact of the prevalence of X, magnitude of the effect between X and T, and magnitude of the effect between X and Y on the bias of the observed exposure effect."
#| echo: false
tar_read(fig_bias)
```


```{r}
#| label: fig-variance
#| fig-cap: "Impact of the prevalence of X, magnitude of the effect between X and T, and magnitude of the effect between X and Y on the variability in the observed exposure effect."
#| echo: false
tar_read(fig_variance)
```

## Application

Haven't discussed, but should be easy enough to find an application to flesh out these ideas? Could maybe use something classic like NHANES.

## Discussion

Add here.
